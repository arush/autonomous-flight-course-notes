1
00:00:00,000 --> 00:00:04,259
Now, we've assumed so far that our random variables are scalars,

2
00:00:04,259 --> 00:00:06,870
but what if they are actually vectors.

3
00:00:06,870 --> 00:00:10,429
The vehicle position isn't going to be given by a single variable x,

4
00:00:10,429 --> 00:00:13,265
It's going to be given by x, y and z.

5
00:00:13,265 --> 00:00:16,410
Presumably, we could fold attitude in there as well.

6
00:00:16,410 --> 00:00:18,570
Which would make x, a six dimensional vector.

7
00:00:18,570 --> 00:00:22,329
And we'd like to have a distribution over those variables.

8
00:00:22,329 --> 00:00:24,699
If we treat each variable separately,

9
00:00:24,699 --> 00:00:26,759
then we're making some strong assumptions about

10
00:00:26,760 --> 00:00:29,705
independence that we'll talk about in a little bit.

11
00:00:29,704 --> 00:00:35,104
But, we can actually define vector value functions over the vector value inputs.

12
00:00:35,104 --> 00:00:38,869
And the same concepts that we saw for scalar values existence before.

13
00:00:38,869 --> 00:00:41,750
The integral still has to come to one.

14
00:00:41,750 --> 00:00:44,310
In this case, the normalization constant that makes this

15
00:00:44,310 --> 00:00:46,760
come to one would be the integral of f(x),

16
00:00:46,759 --> 00:00:50,159
dx1, dx2 up to dxn.

17
00:00:50,159 --> 00:00:53,099
The mean it's given by the same set of integrals,

18
00:00:53,100 --> 00:00:56,520
but it's the integral of x times f(x).

19
00:00:56,520 --> 00:00:57,874
Instead of a variance,

20
00:00:57,874 --> 00:00:59,109
we have a co-variance,

21
00:00:59,109 --> 00:01:01,679
which is going to be given in this case by upper case

22
00:01:01,679 --> 00:01:06,045
Sigma and that's going to be equal to the integral of x minus mu,

23
00:01:06,045 --> 00:01:09,829
times x minus mu transpose, times the density.

24
00:01:09,829 --> 00:01:12,840
This co-variance is going to be a matrix,

25
00:01:12,840 --> 00:01:16,299
because the outer product is going to result in a matrix as well.

26
00:01:16,299 --> 00:01:20,849
The co-variance is going to be a symmetric matrix and positive semi-definite.

27
00:01:20,849 --> 00:01:22,789
If you remember your linear algebra,

28
00:01:22,790 --> 00:01:27,360
positive semi-definite means that there are no negative eigenvalues.

29
00:01:27,359 --> 00:01:31,185
Now, we can define a multivariate Gaussian.

30
00:01:31,185 --> 00:01:34,379
We have to again take care to do the vector math correctly,

31
00:01:34,379 --> 00:01:37,364
but the functional form is not that different.

32
00:01:37,364 --> 00:01:40,199
We're no longer taking square root of two pi,

33
00:01:40,200 --> 00:01:43,105
it's n over two for a vector x of length n.

34
00:01:43,105 --> 00:01:45,855
This term here is the determinant to the co-variance

35
00:01:45,855 --> 00:01:49,260
and then we have the vector value multiplication.

36
00:01:49,260 --> 00:01:52,950
The estimate of the parameters is almost the same as before,

37
00:01:52,950 --> 00:01:55,085
taking care to do the vector math correctly.

38
00:01:55,084 --> 00:01:58,904
And we see here an example of a two dimensional co-variance.

39
00:01:58,905 --> 00:02:05,370
And an important property of the co-variance, is that the eigenvalues and eigenvectors of the co-variance,

40
00:02:05,370 --> 00:02:09,064
describe the amount and direction of uncertainty.

41
00:02:09,064 --> 00:02:12,180
Here, we have an overhead view and

42
00:02:12,180 --> 00:02:15,510
that location at the center of the ellipse would be the mean.

43
00:02:15,509 --> 00:02:20,159
If we take the co-variance and extract the eigenvalues and eigenvectors,

44
00:02:20,159 --> 00:02:24,990
this would be the first eigenvector and this will be the second eigenvector.

45
00:02:24,990 --> 00:02:29,955
If I draw an ellipse at one eigenvalue away from the mean along each vector,

46
00:02:29,955 --> 00:02:32,820
then this one eigenvalue ellipse is going to define

47
00:02:32,819 --> 00:02:36,254
a confidence region which in this case is an ellipsoid.

48
00:02:36,254 --> 00:02:39,449
Just as one standard deviation distance away from the mean and

49
00:02:39,449 --> 00:02:43,409
the scalar case defined the 68 percent confidence interval for x,

50
00:02:43,409 --> 00:02:46,280
the ellipse in 2D defines the confidence interval.

51
00:02:46,280 --> 00:02:48,569
The total probability inside the ellipse at

52
00:02:48,569 --> 00:02:51,329
one standard deviation is not 68 percent in this case.

53
00:02:51,330 --> 00:02:53,719
The total probability for a given length of

54
00:02:53,719 --> 00:02:57,629
axis actually depends on the number of variables in the random vector.

55
00:02:57,629 --> 00:03:02,085
But in 2D, 95 percent probability is captured by axis with length,

56
00:03:02,085 --> 00:03:04,879
roughly five times the eigenvalues.

